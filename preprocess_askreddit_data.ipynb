{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxgiLnCSFCnC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!wget http://files.fionn.xyz/Datasets/Reddit/Answers_R.txt\n",
        "!wget http://files.fionn.xyz/Datasets/Reddit/Questions_R.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuXI0XdvFOPs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Q0S2MgMCLG"
      },
      "outputs": [],
      "source": [
        "qns = open(\"./Questions_R.txt\", 'r').read()  \n",
        "ans = open(\"./Answers_R.txt\", 'r').read()\n",
        "\n",
        "from itertools import zip_longest\n",
        "with open(\"./Questions_R.txt\") as f1, open(\"./Answers_R.txt\") as f2, open(\"./Questions_Answers_R.txt\", 'w') as of:\n",
        "    for lines in zip_longest(f1, f2):\n",
        "        for line in lines:\n",
        "            if line is not None: print(line, file=of, end='')\n",
        "\n",
        "tagged_qa = open(\"./Questions_Answers_R.txt\", 'r')\n",
        "\n",
        "count=0\n",
        "output_file =  open('qa_reddit_tagged.txt','w')\n",
        "\n",
        "for line in tagged_qa:\n",
        "    if count%2==0:\n",
        "        output_file.write(\"Question: \"+line+\" \")\n",
        "    else:\n",
        "        output_file.write(\"Answer: \"+line)\n",
        "\n",
        "    count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0K_44YOMWu5"
      },
      "outputs": [],
      "source": [
        "!cat qa_reddit_tagged.txt\n",
        "!split -l  400000 /content/qa_reddit_tagged.txt -d --additional-suffix=.txt\n",
        "!mkdir splits\n",
        "!cp x* ./splits/\n",
        "!zip -r splits.zip ./splits/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2KuGemKQpkB"
      },
      "outputs": [],
      "source": [
        "files.download(\"/content/splits.zip\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NBhyK2uzOuPK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocess-askreddit-data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}